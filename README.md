# Active_Learning_in_NLP_using_Small_Text_Technique

# What is Small-Text? ğŸ¯

Small-Text is a Python library that enables active learningâ€”a method where a model intelligently selects the most informative samples to label from a pool of unlabeled data. This approach is particularly useful when:

â€¢ You have limited labeled data.

â€¢ Labeling new data is expensive or time-consuming.

By reducing the amount of labeled data required, Small-Text makes machine learning projects more efficient! ğŸš€


# Library Setup ğŸ› ï¸

â€¢ Essential libraries like small-text, datasets, sklearn, and visualization tools (seaborn, matplotlib) are imported.

â€¢ Warnings and logs are suppressed to keep the output clean and focused.

 
# Data Loading ğŸ“š

â€¢ The IMDB dataset (movie reviews) is loaded using the ğŸ¤— datasets library.

   â€¢ Train Set: Used for active learning.
   
   â€¢ Test Set: Used for evaluating performance.
   
â€¢ The text column (reviews) and label column (sentiment: 0 for negative, 1 for positive) are extracted.



# Text Representation âœï¸

â€¢ A TF-IDF Vectorizer converts text data into numerical features for the machine learning model.

â€¢ Features: Limited to the top 5000 most important words/phrases.

â€¢ N-grams: Includes single words, pairs of words (bigrams), and triples (trigrams) to capture context.

# Dataset Preparation ğŸ”¢

â€¢ The SklearnDataset class from small-text is used to prepare datasets for training and testing.

â€¢ The vectorizer transforms raw text into feature vectors, while target_labels defines the number of classes.




# Model Initialization ğŸ§ 

â€¢ Logistic Regression is selected as the machine learning model due to its simplicity and efficiency for binary classification tasks.

â€¢ The model is wrapped using the SklearnClassifierFactory, making it compatible with the small-text library.

# Query Strategy ğŸ“ˆ

â€¢ The PredictionEntropy query strategy is chosen.

â€¢ This strategy selects the most uncertain samples for labeling, where the model struggles to make confident predictions.

â€¢ It ensures that labeling efforts focus on the most informative data points.


# Active Learner Initialization ğŸ

â€¢ The Pool-Based Active Learner is initialized with:

  â€¢ The Logistic Regression model.
  â€¢ The PredictionEntropy strategy.
  â€¢ A balanced random initialization of 200 samples, ensuring diverse labeled data to start training.


# Evaluation Function ğŸ§ª

â€¢ A function is defined to compute the weighted F1-score, a metric that balances precision and recall across classes.

â€¢ The model's performance is evaluated on both:

  â€¢ Training set: To measure how well the model learns.
  
  â€¢ Test set: To measure generalization to unseen data.



# Active Learning Loop ğŸ”„ 
â€¢ 50 Iterations are performed to iteratively improve the model:
  1.Querying: The model selects 20 samples for labeling in each iteration.
  2.Updating: The newly labeled samples are added to the training data, and the model is retrained.
  3.Evaluating: The performance is assessed, and F1-scores are recorded.


# Visualization ğŸ“Š

â€¢ A line plot and scatter plot visualize how the F1-score evolves over iterations.
â€¢ The chart shows:
  â€¢ X-axis: Query iteration number.
  â€¢ Y-axis: Classification F1-score.
â€¢ Seaborn styling ensures a clean and professional appearance.



#  Why Active Learning Matters! âœ¨
## Smart Labeling ğŸ’¡ 
Focus only on important samples for labeling, saving time and resources.

## Incremental Improvement ğŸ“ˆ
Observe consistent performance gains as the model learns with fewer labeled samples.

## NLP Applications ğŸ’»

This method is perfect for tasks like sentiment analysis, text classification, and more!

With Small-Text, you can build smarter, faster, and more efficient NLP models. ğŸš€âœ¨
